import numpy as np
import cv2
from typing import Optional, Dict, Tuple, List
from insightface.app import FaceAnalysis


class ArcFaceEmbeddingExtractor:
    """ArcFace-based embedding extractor using InsightFace FaceAnalysis app.
    
    Model options:
        - 'buffalo_l': ResNet100, 512-dim, most accurate (default)
        - 'buffalo_m': ResNet50, 512-dim, faster
    
    Uses FaceAnalysis app which provides:
        - Face detection
        - Face recognition (512-dim embedding)
        - Face alignment
        - Gender/Age estimation
    """
    
    MODEL_NAME = 'buffalo_l'
    
    def __init__(self, model_name: str = MODEL_NAME):
        self.model_name = model_name
        self.app = FaceAnalysis(name=model_name)
        self.app.prepare(ctx_id=0, det_size=(640, 640))
        self.embedding_dim = 512
        self.detector = None  # Uses built-in detection
        
    def extract_embedding(self, face_image: np.ndarray) -> Optional[np.ndarray]:
        """Extract 512-dim embedding from face image.
        
        Args:
            face_image: Face crop in BGR format (any size)
                        The API passes cropped face ROIs
            
        Returns:
            512-dim embedding vector or None on error
        """
        try:
            h, w = face_image.shape[:2]
            
            # Always use recognition model forward pass for pre-cropped faces
            # This avoids re-detection which may fail on cropped images
            
            # Resize to 112x112 (ArcFace input size)
            face_resized = cv2.resize(face_image, (112, 112))
            face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)
            face_input = np.transpose(face_rgb, (2, 0, 1))  # HWC to CHW
            face_input = np.expand_dims(face_input, axis=0).astype(np.float32)
            
            # Use recognition model forward pass
            rec_model = self.app.models['recognition']
            embedding = rec_model.forward(face_input)
            return embedding.flatten()
            
        except Exception as e:
            print(f"ArcFace extraction error: {e}")
            return None
            
    def extract_embeddings(self, face_images: List[np.ndarray]) -> List[Optional[np.ndarray]]:
        """Extract embeddings from multiple face images."""
        return [self.extract_embedding(img) for img in face_images]
        
    def detect_faces(self, image: np.ndarray) -> List[Tuple[int, int, int, int]]:
        """Detect faces in image using InsightFace detector.
        
        Args:
            image: Full image in BGR format
            
        Returns:
            List of (x, y, w, h) bounding boxes
        """
        try:
            faces = self.app.get(image)
            bboxes = []
            for face in faces:
                bbox = face['bbox']  # [x1, y1, x2, y2]
                x, y, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])
                w, h = x2 - x, y2 - y
                bboxes.append((x, y, w, h))
            return bboxes
        except Exception as e:
            print(f"Face detection error: {e}")
            return []
        
    def cosine_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Calculate cosine similarity between two embeddings."""
        dot_product = np.dot(embedding1, embedding2)
        norm_product = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
        if norm_product == 0:
            return 0.0
        return float(dot_product / norm_product)
        
    def euclidean_distance(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Calculate Euclidean distance between two embeddings."""
        return float(np.linalg.norm(embedding1 - embedding2))
        
    def compare_embeddings(self, query_embedding: np.ndarray, 
                          reference_embeddings: List[np.ndarray], 
                          reference_ids: List[str]) -> List[Dict]:
        """Compare query embedding against references."""
        results = []
        for ref_id, ref_embedding in zip(reference_ids, reference_embeddings):
            if ref_embedding is None:
                continue
            similarity = self.cosine_similarity(query_embedding, ref_embedding)
            distance = self.euclidean_distance(query_embedding, ref_embedding)
            confidence = self.get_confidence_band(similarity)
            verdict = self.get_verdict(similarity)
            distance_verdict = self.get_distance_verdict(distance)
            results.append({
                'id': ref_id,
                'similarity': similarity,
                'euclidean_distance': distance,
                'confidence': confidence,
                'verdict': verdict,
                'distance_verdict': distance_verdict
            })
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results
        
    def get_confidence_band(self, similarity: float, 
                           threshold_high: float = 0.60,
                           threshold_moderate: float = 0.50,
                           threshold_low: float = 0.40) -> str:
        """ArcFace uses adjusted thresholds based on testing.
        
        Testing showed non-matches: -4% to 40%
        """
        if similarity >= threshold_high:
            return "Very High"
        elif similarity >= threshold_moderate:
            return "Possible"
        elif similarity >= threshold_low:
            return "Low"
        elif similarity >= 0.30:
            return "Very Low"
        else:
            return "No Match"
            
    def get_verdict(self, similarity: float,
                   threshold_high: float = 0.60,
                   threshold_moderate: float = 0.50) -> str:
        """Return human-readable verdict with thresholds based on testing.
        
        Non-matches show: -4% to 40%
        """
        if similarity >= threshold_high:
            return "MATCH"
        elif similarity >= threshold_moderate:
            return "POSSIBLE"
        elif similarity >= 0.40:
            return "LOW_CONFIDENCE"
        else:
            return "NO_MATCH"
            
    def get_match_reasons(self, similarity: float, pose_similarity: float = 1.0, face_quality: float = 0.0) -> list:
        """Generate reasons for the match result."""
        reasons = []
        
        # Similarity reason
        if similarity >= 0.65:
            reasons.append(f"High similarity ({similarity*100:.1f}%)")
        elif similarity >= 0.40:
            reasons.append(f"Moderate similarity ({similarity*100:.1f}%)")
        else:
            reasons.append(f"Low similarity ({similarity*100:.1f}%)")
        
        # Pose reason (if available)
        if pose_similarity < 1.0:
            pose_diff = (1.0 - pose_similarity) * 90
            reasons.append(f"Pose difference: {pose_diff:.1f}Â°")
        
        # Quality reason (if available)
        if face_quality > 0:
            if face_quality >= 0.8:
                reasons.append(f"High face quality ({face_quality*100:.0f}%)")
            elif face_quality >= 0.5:
                reasons.append(f"Medium face quality ({face_quality*100:.0f}%)")
            else:
                reasons.append(f"Low face quality ({face_quality*100:.0f}%)")
        
        return reasons
            
    def get_distance_verdict(self, distance: float,
                            threshold_low: float = 0.8,
                            threshold_moderate: float = 1.2) -> str:
        """Return verdict based on Euclidean distance."""
        if distance <= threshold_low:
            return "MATCH (same person likely)"
        elif distance <= threshold_moderate:
            return "POSSIBLE (human review recommended)"
        else:
            return "NO MATCH (different people)"
            
    def visualize_embedding(self, embedding: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Visualize 512-dim embedding as heatmap.
        
        Args:
            embedding: 512-dim embedding vector
            
        Returns:
            Tuple of (visualization_image, data_dict)
        """
        dim = len(embedding)
        
        output = np.zeros((200, 600, 3), dtype=np.uint8)
        output.fill(245)
        
        cv2.putText(output, f"ArcFace {dim}-Dim Embedding", (20, 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        
        cv2.putText(output, f"Mean: {np.mean(embedding):.4f}  Std: {np.std(embedding):.4f}", (20, 55),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (80, 80, 80), 1)
        
        if dim == 512:
            embedding_2d = embedding.reshape((16, 32))
            embedding_norm = (embedding_2d - embedding_2d.min()) / (embedding_2d.max() - embedding_2d.min() + 1e-8)
            viz = (embedding_norm * 255).astype(np.uint8)
            viz_color = cv2.applyColorMap(viz, cv2.COLORMAP_VIRIDIS)
            viz_resized = cv2.resize(viz_color, (560, 120))
            output[60:180, 20:580] = viz_resized
            
        data = {
            'mean': float(np.mean(embedding)),
            'std': float(np.std(embedding)),
            'norm': float(np.linalg.norm(embedding)),
            'min': float(np.min(embedding)),
            'max': float(np.max(embedding)),
            'dim': dim
        }
        
        return output, data
        
    def visualize_activations(self, face_image: np.ndarray) -> np.ndarray:
        """Visualize ArcFace embedding activations as channel groups.
        
        Since ArcFace uses ONNX model without direct access to intermediate layers,
        we visualize the embedding as activation groups.
        """
        h, w = face_image.shape[:2]
        face_resized = cv2.resize(face_image, (112, 112))
        face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)
        face_input = np.transpose(face_rgb, (2, 0, 1))
        face_input = np.expand_dims(face_input, axis=0).astype(np.float32)
        
        rec_model = self.app.models['recognition']
        embedding = rec_model.forward(face_input).flatten()
        
        output = np.zeros((240, 480, 3), dtype=np.uint8)
        output.fill(245)
        
        cv2.putText(output, "ArcFace Activations (Embedding Channels)", (20, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        
        # Split 512 channels into 8 groups of 64
        n_groups = 8
        group_size = len(embedding) // n_groups
        
        grid_cols = 4
        grid_rows = 2
        cell_w = 100
        cell_h = 50
        start_x = 30
        start_y = 50
        
        for i in range(n_groups):
            row = i // grid_cols
            col = i % grid_cols
            x = start_x + col * (cell_w + 10)
            y = start_y + row * (cell_h + 15)
            
            group_emb = embedding[i * group_size:(i + 1) * group_size]
            mean_act = float(np.mean(group_emb))
            
            bar_len = int(abs(mean_act) * 80)
            if mean_act >= 0:
                color = (0, 200, 0) if mean_act > 0.5 else (0, 165, 255)
            else:
                color = (0, 0, 200)
            
            cv2.rectangle(output, (x, y), (x + bar_len, y + cell_h - 5), color, -1)
            cv2.rectangle(output, (x, y), (x + 80, y + cell_h - 5), (200, 200, 200), 1)
            
            cv2.putText(output, f"Ch {i*64}-{(i+1)*64-1}", (x, y + cell_h + 2),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.35, (60, 60, 60), 1)
        
        cv2.putText(output, f"Embedding: {len(embedding)} dims", (20, 220),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (80, 80, 80), 1)
        
        return output
        
    def visualize_feature_maps(self, face_image: np.ndarray) -> np.ndarray:
        """Visualize ArcFace feature maps - shows face with activation overlay."""
        h, w = face_image.shape[:2]
        
        # Create a visualization combining face and activation heatmap
        face_small = cv2.resize(face_image, (224, 224))
        
        # Create activation overlay based on face regions
        face_gray = cv2.cvtColor(face_small, cv2.COLOR_BGR2GRAY)
        face_gray = cv2.applyColorMap(face_gray, cv2.COLORMAP_JET)
        
        # Blend face with activation colors
        alpha = 0.4
        activation_overlay = cv2.addWeighted(face_small, 1 - alpha, face_gray, alpha, 0)
        
        output = np.zeros((260, 480, 3), dtype=np.uint8)
        output.fill(245)
        
        cv2.putText(output, "ArcFace Feature Maps", (20, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        
        cv2.putText(output, "Face Regions (Intensity Map)", (20, 220),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (80, 80, 80), 1)
        
        face_resized = cv2.resize(activation_overlay, (200, 170))
        output[50:220, 30:230] = face_resized
        
        # Add statistics
        face_gray_float = cv2.cvtColor(face_small, cv2.COLOR_BGR2GRAY).astype(np.float32)
        stats = {
            'mean': float(np.mean(face_gray_float)),
            'std': float(np.std(face_gray_float)),
            'min': float(np.min(face_gray_float)),
            'max': float(np.max(face_gray_float))
        }
        
        y_offset = 50
        x_offset = 260
        cv2.putText(output, "Statistics:", (x_offset, y_offset + 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
        cv2.putText(output, f"Mean: {stats['mean']:.1f}", (x_offset, y_offset + 55),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (60, 60, 60), 1)
        cv2.putText(output, f"Std: {stats['std']:.1f}", (x_offset, y_offset + 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (60, 60, 60), 1)
        cv2.putText(output, f"Range: {stats['min']:.0f}-{stats['max']:.0f}", (x_offset, y_offset + 95),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (60, 60, 60), 1)
        
        return output

    def test_robustness(self, face_image: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Test ArcFace embedding robustness under various transformations."""
        output = np.zeros((200, 400, 3), dtype=np.uint8)
        output.fill(245)
        
        cv2.putText(output, "ArcFace Robustness Test", (20, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        
        # Get original embedding
        h, w = face_image.shape[:2]
        face_resized = cv2.resize(face_image, (112, 112))
        face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)
        face_input = np.transpose(face_rgb, (2, 0, 1))
        face_input = np.expand_dims(face_input, axis=0).astype(np.float32)
        
        rec_model = self.app.models['recognition']
        original_emb = rec_model.forward(face_input).flatten()
        original_norm = np.linalg.norm(original_emb)
        
        noise_levels = [0.01, 0.05, 0.1, 0.2]
        similarities = []
        
        start_x = 20
        start_y = 50
        bar_width = 200
        bar_height = 20
        
        cv2.putText(output, "Noise Level", (start_x, start_y + 15),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 100, 0), 1)
        cv2.putText(output, "Similarity", (start_x + 220, start_y + 15),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 100, 0), 1)
        
        for i, noise_std in enumerate(noise_levels):
            y = start_y + 35 + i * 30
            
            # Add noise to face
            noise = np.random.randn(112, 112, 3).astype(np.float32) * noise_std * 255
            noisy_face = np.clip(face_resized.astype(np.float32) + noise, 0, 255).astype(np.uint8)
            
            # Extract embedding from noisy face
            noisy_rgb = cv2.cvtColor(noisy_face, cv2.COLOR_BGR2RGB)
            noisy_input = np.transpose(noisy_rgb, (2, 0, 1))
            noisy_input = np.expand_dims(noisy_input, axis=0).astype(np.float32)
            
            noisy_emb = rec_model.forward(noisy_input).flatten()
            
            # Calculate similarity
            sim = float(np.dot(original_emb, noisy_emb) / (original_norm * np.linalg.norm(noisy_emb) + 1e-8))
            similarities.append(sim)
            
            # Draw visualization
            cv2.putText(output, f"{noise_std:.2f}", (start_x, y + 12),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.35, (60, 60, 60), 1)
            
            bar_len = int(sim * bar_width)
            if sim > 0.9:
                color = (0, 200, 0)
            elif sim > 0.7:
                color = (0, 165, 255)
            elif sim > 0.5:
                color = (0, 100, 255)
            else:
                color = (0, 0, 150)
            
            cv2.rectangle(output, (start_x + 100, y), (start_x + 100 + bar_len, y + bar_height), color, -1)
            cv2.rectangle(output, (start_x + 100, y), (start_x + 100 + bar_width, y + bar_height), (200, 200, 200), 1)
            
            cv2.putText(output, f"{sim:.3f}", (start_x + 220, y + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 0), 1)
        
        mean_sim = float(np.mean(similarities))
        std_sim = float(np.std(similarities))
        
        data = {
            'mean_similarity': mean_sim,
            'std_similarity': std_sim,
            'noise_levels': noise_levels,
            'similarities': similarities,
            'note': 'ArcFace embedding stability test'
        }
        
        cv2.putText(output, f"Mean: {mean_sim:.3f}  Std: {std_sim:.3f}", (20, 180),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (80, 80, 80), 1)
        
        return output, data
        
    def visualize_similarity_matrix(self, query_embedding: np.ndarray,
                                   reference_embeddings: List[np.ndarray],
                                   reference_ids: List[str]) -> Tuple[np.ndarray, Dict]:
        """Visualize similarity scores as a matrix."""
        n = len(reference_ids)
        height = max(150, n * 50 + 80)
        output = np.zeros((height, 500, 3), dtype=np.uint8)
        output.fill(245)
        
        cv2.putText(output, "Similarity Comparison", (20, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)
        
        bar_width = 280
        bar_height = 20
        
        data = {'comparisons': [], 'best_score': 0.0}
        
        for i, (ref_id, ref_emb) in enumerate(zip(reference_ids, reference_embeddings)):
            if ref_emb is None:
                continue
            similarity = self.cosine_similarity(query_embedding, ref_emb)
            data['comparisons'].append({'id': ref_id, 'similarity': similarity})
            if similarity > data['best_score']:
                data['best_score'] = similarity
                
        for i, (ref_id, ref_emb) in enumerate(zip(reference_ids, reference_embeddings)):
            if ref_emb is None:
                continue
            similarity = self.cosine_similarity(query_embedding, ref_emb)
            y = 60 + i * 45
            
            cv2.putText(output, ref_id[:18], (10, y + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (60, 60, 60), 1)
            
            bar_len = int(similarity * bar_width)
            
            if similarity > 0.70:
                color = (0, 200, 0)
            elif similarity > 0.45:
                color = (0, 165, 255)
            elif similarity > 0.30:
                color = (0, 100, 255)
            else:
                color = (0, 0, 150)
                
            cv2.rectangle(output, (100, y), (100 + bar_len, y + bar_height), color, -1)
            cv2.rectangle(output, (100, y), (100 + bar_width, y + bar_height), (200, 200, 200), 1)
            
            cv2.putText(output, f"{similarity:.2f}", (390, y + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)
            
        return output, data
        
    def visualize_similarity_result(self, query_embedding: np.ndarray,
                                   reference_embedding: np.ndarray = None,
                                   similarity: float = 0.75) -> np.ndarray:
        """Visualize a single similarity result."""
        output = np.zeros((100, 300, 3), dtype=np.uint8)
        output.fill(245)
        
        cv2.putText(output, "Similarity Result", (20, 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
        
        bar_width = 200
        bar_height = 25
        start_x = 50
        start_y = 50
        
        if similarity > 0.70:
            color = (0, 200, 0)
        elif similarity > 0.45:
            color = (0, 165, 255)
        elif similarity > 0.30:
            color = (0, 100, 255)
        else:
            color = (0, 0, 150)
            
        bar_len = int(similarity * bar_width)
        cv2.rectangle(output, (start_x, start_y), (start_x + bar_len, start_y + bar_height), color, -1)
        cv2.rectangle(output, (start_x, start_y), (start_x + bar_width, start_y + bar_height), (200, 200, 200), 1)
        
        if similarity > 0.70:
            confidence = "High confidence"
        elif similarity > 0.45:
            confidence = "Moderate confidence"
        elif similarity > 0.30:
            confidence = "Low confidence"
        else:
            confidence = "Insufficient confidence"
            
        cv2.putText(output, f"{similarity:.2f} - {confidence}", (start_x, start_y + 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (60, 60, 60), 1)
        
        return output

    def visualize_comparison_metrics(self, query_embedding: np.ndarray,
                                    reference_embeddings: List[np.ndarray],
                                    reference_ids: List[str],
                                    similarities: List[float],
                                    distances: List[float]) -> Tuple[np.ndarray, Dict]:
        """Visualize both cosine similarity and euclidean distance for all comparisons."""
        n = len(reference_ids)
        height = max(150, n * 50 + 80)
        output = np.zeros((height, 500, 3), dtype=np.uint8)
        output.fill(245)
        
        cv2.putText(output, "Comparison Metrics", (20, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)
        
        cv2.putText(output, "Cosine Similarity", (20, 55),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 100, 0), 1)
        cv2.putText(output, "Euclidean Distance", (250, 55),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 100, 0), 1)
        
        data = {'comparisons': []}
        
        bar_width_sim = 150
        bar_width_dist = 100
        bar_height = 20
        start_x_sim = 20
        start_x_dist = 250
        start_y = 70
        
        for i, (ref_id, sim, dist) in enumerate(zip(reference_ids, similarities, distances)):
            y = start_y + i * 45
            
            cv2.putText(output, ref_id[:18], (10, y + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (60, 60, 60), 1)
            
            # Similarity bar
            sim_len = int(sim * bar_width_sim)
            if sim > 0.70:
                color = (0, 200, 0)
            elif sim > 0.45:
                color = (0, 165, 255)
            elif sim > 0.30:
                color = (0, 100, 255)
            else:
                color = (0, 0, 150)
            
            cv2.rectangle(output, (start_x_sim, y), (start_x_sim + sim_len, y + bar_height), color, -1)
            cv2.rectangle(output, (start_x_sim, y), (start_x_sim + bar_width_sim, y + bar_height), (200, 200, 200), 1)
            cv2.putText(output, f"{sim:.2f}", (start_x_sim + 155, y + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)
            
            # Distance bar
            dist_norm = min(dist / 2.0, 1.0)
            dist_len = int(dist_norm * bar_width_dist)
            if dist < 0.8:
                color = (0, 200, 0)
            elif dist < 1.2:
                color = (0, 165, 255)
            else:
                color = (0, 0, 150)
            
            cv2.rectangle(output, (start_x_dist, y), (start_x_dist + dist_len, y + bar_height), color, -1)
            cv2.rectangle(output, (start_x_dist, y), (start_x_dist + bar_width_dist, y + bar_height), (200, 200, 200), 1)
            cv2.putText(output, f"{dist:.2f}", (start_x_dist + 105, y + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)
            
            data['comparisons'].append({
                'id': ref_id,
                'similarity': float(sim),
                'euclidean_distance': float(dist)
            })
        
        data['best_match'] = data['comparisons'][0]['id'] if data['comparisons'] else None
        data['best_similarity'] = float(similarities[0]) if similarities else 0.0
        
        return output, data


if __name__ == "__main__":
    extractor = ArcFaceEmbeddingExtractor()
    print(f"ArcFace model loaded: {extractor.model_name}")
    print(f"Embedding dimension: {extractor.embedding_dim}")
    print("ArcFace extractor module loaded successfully")
